"""Training pipeline untuk ML models."""

import os
import json
import pickle
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import numpy as np
import pandas as pd
from pathlib import Path
import warnings

try:
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    import mlflow
    import mlflow.sklearn
    import mlflow.tensorflow
except ImportError:
    print("Some ML libraries not installed")

from app.config.database import db_manager
from app.config.logging import ml_logger
from app.config.settings import settings
from app.config.redis import redis_manager
from app.preprocessing.data_processor import DataProcessor
from app.features.engineering import FeatureEngineer
from app.models.prophet_model import ProphetForecaster
from app.models.lstm_model import LSTMForecaster
from app.models.anomaly_detection import AnomalyDetector

warnings.filterwarnings('ignore')


class ModelTrainer:
    """Centralized model training pipeline."""
    
    def __init__(self):
        self.data_processor = DataProcessor()
        self.feature_engineer = FeatureEngineer()
        self.models = {}
        self.training_results = {}
        
        # Setup MLflow if configured
        self._setup_mlflow()
        
        # Ensure model directories exist
        self._setup_directories()
    
    def _setup_mlflow(self):
        """Setup MLflow tracking."""
        try:
            if settings.mlflow_tracking_uri:
                mlflow.set_tracking_uri(settings.mlflow_tracking_uri)
                mlflow.set_experiment(settings.mlflow_experiment_name)
                ml_logger.info("MLflow tracking configured", uri=settings.mlflow_tracking_uri)
        except Exception as e:
            ml_logger.warning("Failed to setup MLflow", error=str(e))
    
    def _setup_directories(self):
        """Create necessary directories."""
        directories = [
            settings.model_store_path,
            settings.artifact_store_path,
            settings.feature_store_path
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
    
    def train_commodity_models(
        self,
        commodity_code: str,
        region_codes: List[str] = None,
        model_types: List[str] = ['prophet', 'lstm'],
        start_date: str = None,
        end_date: str = None,
        retrain: bool = False
    ) -> Dict:
        """Train models for a specific commodity."""
        
        try:
            ml_logger.info(
                "Starting commodity model training",
                commodity_code=commodity_code,
                region_codes=region_codes,
                model_types=model_types
            )
            
            # Load and preprocess data
            price_data, weather_data = self.data_processor.load_and_preprocess_data(\n                commodity_codes=[commodity_code],\n                region_codes=region_codes,\n                start_date=start_date,\n                end_date=end_date,\n                include_weather=True\n            )\n            \n            if price_data.empty:\n                raise ValueError(f\"No data available for commodity {commodity_code}\")\n            \n            # Engineer features\n            features_df = self.feature_engineer.engineer_features(\n                price_data=price_data,\n                weather_data=weather_data,\n                commodity_code=commodity_code\n            )\n            \n            # Cache features\n            await self._cache_features(commodity_code, features_df)\n            \n            # Train models by region or aggregate\n            training_results = {}\n            \n            if region_codes:\n                # Train separate models for each region\n                for region_code in region_codes:\n                    region_data = features_df[\n                        features_df['region_code'] == region_code\n                    ].copy()\n                    \n                    if not region_data.empty:\n                        region_results = self._train_region_models(\n                            commodity_code=commodity_code,\n                            region_code=region_code,\n                            data=region_data,\n                            model_types=model_types,\n                            retrain=retrain\n                        )\n                        training_results[region_code] = region_results\n            else:\n                # Train aggregated models\n                aggregated_results = self._train_region_models(\n                    commodity_code=commodity_code,\n                    region_code=None,\n                    data=features_df,\n                    model_types=model_types,\n                    retrain=retrain\n                )\n                training_results['national'] = aggregated_results\n            \n            # Train anomaly detection models\n            if 'anomaly' in model_types or len(model_types) == 0:\n                anomaly_results = self._train_anomaly_models(\n                    commodity_code=commodity_code,\n                    data=features_df,\n                    retrain=retrain\n                )\n                training_results['anomaly'] = anomaly_results\n            \n            # Store training results\n            self.training_results[commodity_code] = training_results\n            \n            # Log overall results\n            ml_logger.info(\n                \"Commodity model training completed\",\n                commodity_code=commodity_code,\n                training_results=self._summarize_training_results(training_results)\n            )\n            \n            return training_results\n            \n        except Exception as e:\n            ml_logger.error(\n                \"Commodity model training failed\",\n                commodity_code=commodity_code,\n                error=str(e)\n            )\n            raise e\n    \n    def _train_region_models(\n        self,\n        commodity_code: str,\n        region_code: str,\n        data: pd.DataFrame,\n        model_types: List[str],\n        retrain: bool = False\n    ) -> Dict:\n        \"\"\"Train models for a specific commodity-region combination.\"\"\"\n        \n        results = {}\n        \n        # Create train/test split\n        train_data, test_data = self.data_processor.create_train_test_split(\n            data, test_size=settings.validation_split_ratio, time_based=True\n        )\n        \n        # Train Prophet model\n        if 'prophet' in model_types:\n            try:\n                prophet_results = self._train_prophet_model(\n                    commodity_code=commodity_code,\n                    region_code=region_code,\n                    train_data=train_data,\n                    test_data=test_data,\n                    retrain=retrain\n                )\n                results['prophet'] = prophet_results\n            except Exception as e:\n                ml_logger.error(\"Prophet training failed\", error=str(e))\n                results['prophet'] = {'error': str(e)}\n        \n        # Train LSTM model\n        if 'lstm' in model_types:\n            try:\n                lstm_results = self._train_lstm_model(\n                    commodity_code=commodity_code,\n                    region_code=region_code,\n                    train_data=train_data,\n                    test_data=test_data,\n                    retrain=retrain\n                )\n                results['lstm'] = lstm_results\n            except Exception as e:\n                ml_logger.error(\"LSTM training failed\", error=str(e))\n                results['lstm'] = {'error': str(e)}\n        \n        return results\n    \n    def _train_prophet_model(\n        self,\n        commodity_code: str,\n        region_code: str,\n        train_data: pd.DataFrame,\n        test_data: pd.DataFrame,\n        retrain: bool = False\n    ) -> Dict:\n        \"\"\"Train Prophet model.\"\"\"\n        \n        model_key = f\"prophet_{commodity_code}_{region_code or 'national'}\"\n        model_path = os.path.join(settings.model_store_path, f\"{model_key}.pkl\")\n        \n        # Check if model exists and retrain is not forced\n        if os.path.exists(model_path) and not retrain:\n            ml_logger.info(\"Prophet model already exists, skipping training\", model_key=model_key)\n            return {'status': 'skipped', 'reason': 'model_exists'}\n        \n        # Start MLflow run\n        with mlflow.start_run(run_name=f\"prophet_{commodity_code}_{region_code}\", nested=True):\n            # Initialize model\n            prophet_model = ProphetForecaster(\n                commodity_code=commodity_code,\n                region_code=region_code\n            )\n            \n            # Train model\n            training_metrics = prophet_model.fit(train_data, target_col='price')\n            \n            # Validate model\n            validation_metrics = self._validate_prophet_model(\n                prophet_model, test_data\n            )\n            \n            # Save model\n            prophet_model.save_model(model_path)\n            \n            # Log metrics to MLflow\n            mlflow.log_params({\n                'model_type': 'prophet',\n                'commodity_code': commodity_code,\n                'region_code': region_code or 'national',\n                'seasonality_mode': prophet_model.seasonality_mode,\n                'training_samples': len(train_data)\n            })\n            \n            mlflow.log_metrics(training_metrics)\n            mlflow.log_metrics({f\"val_{k}\": v for k, v in validation_metrics.items()})\n            \n            # Store model in registry\n            self.models[model_key] = prophet_model\n            \n            results = {\n                'status': 'success',\n                'model_path': model_path,\n                'training_metrics': training_metrics,\n                'validation_metrics': validation_metrics,\n                'model_info': prophet_model.get_model_info()\n            }\n            \n            return results\n    \n    def _train_lstm_model(\n        self,\n        commodity_code: str,\n        region_code: str,\n        train_data: pd.DataFrame,\n        test_data: pd.DataFrame,\n        retrain: bool = False\n    ) -> Dict:\n        \"\"\"Train LSTM model.\"\"\"\n        \n        model_key = f\"lstm_{commodity_code}_{region_code or 'national'}\"\n        model_path = os.path.join(settings.model_store_path, f\"{model_key}.pkl\")\n        \n        # Check if model exists and retrain is not forced\n        if os.path.exists(model_path) and not retrain:\n            ml_logger.info(\"LSTM model already exists, skipping training\", model_key=model_key)\n            return {'status': 'skipped', 'reason': 'model_exists'}\n        \n        # Start MLflow run\n        with mlflow.start_run(run_name=f\"lstm_{commodity_code}_{region_code}\", nested=True):\n            # Initialize model\n            lstm_model = LSTMForecaster(\n                commodity_code=commodity_code,\n                region_code=region_code\n            )\n            \n            # Prepare features\n            feature_cols = self.feature_engineer.get_feature_importance_names(train_data)\n            \n            # Train model\n            training_metrics = lstm_model.fit(\n                train_data,\n                target_col='price',\n                feature_cols=feature_cols,\n                validation_split=0.2,\n                verbose=0\n            )\n            \n            # Validate model\n            validation_metrics = lstm_model.evaluate_model(test_data, target_col='price')\n            \n            # Save model\n            lstm_model.save_model(model_path)\n            \n            # Log metrics to MLflow\n            mlflow.log_params({\n                'model_type': 'lstm',\n                'commodity_code': commodity_code,\n                'region_code': region_code or 'national',\n                'sequence_length': lstm_model.sequence_length,\n                'hidden_units': lstm_model.hidden_units,\n                'training_samples': len(train_data)\n            })\n            \n            mlflow.log_metrics(training_metrics)\n            mlflow.log_metrics(validation_metrics)\n            \n            # Store model in registry\n            self.models[model_key] = lstm_model\n            \n            results = {\n                'status': 'success',\n                'model_path': model_path,\n                'training_metrics': training_metrics,\n                'validation_metrics': validation_metrics,\n                'model_info': lstm_model.get_model_info()\n            }\n            \n            return results\n    \n    def _train_anomaly_models(\n        self,\n        commodity_code: str,\n        data: pd.DataFrame,\n        retrain: bool = False\n    ) -> Dict:\n        \"\"\"Train anomaly detection models.\"\"\"\n        \n        model_key = f\"anomaly_{commodity_code}\"\n        model_path = os.path.join(settings.model_store_path, f\"{model_key}.pkl\")\n        \n        # Check if model exists and retrain is not forced\n        if os.path.exists(model_path) and not retrain:\n            ml_logger.info(\"Anomaly model already exists, skipping training\", model_key=model_key)\n            return {'status': 'skipped', 'reason': 'model_exists'}\n        \n        # Start MLflow run\n        with mlflow.start_run(run_name=f\"anomaly_{commodity_code}\", nested=True):\n            # Initialize detector\n            anomaly_detector = AnomalyDetector(\n                commodity_code=commodity_code\n            )\n            \n            # Train detector\n            training_results = anomaly_detector.fit(\n                data, detection_types=['temporal', 'geographic']\n            )\n            \n            # Test anomaly detection\n            detected_anomalies = anomaly_detector.detect_anomalies(\n                data, detection_types=['temporal', 'geographic'], sensitivity=0.1\n            )\n            \n            # Save model\n            with open(model_path, 'wb') as f:\n                pickle.dump(anomaly_detector, f)\n            \n            # Log metrics to MLflow\n            mlflow.log_params({\n                'model_type': 'anomaly_detection',\n                'commodity_code': commodity_code,\n                'detection_types': ['temporal', 'geographic'],\n                'training_samples': len(data)\n            })\n            \n            mlflow.log_metrics({\n                'detected_anomalies': len(detected_anomalies),\n                'anomaly_rate': len(detected_anomalies) / len(data) * 100\n            })\n            \n            # Store model in registry\n            self.models[model_key] = anomaly_detector\n            \n            results = {\n                'status': 'success',\n                'model_path': model_path,\n                'training_results': training_results,\n                'detected_anomalies_count': len(detected_anomalies),\n                'anomaly_rate': len(detected_anomalies) / len(data) * 100\n            }\n            \n            return results\n    \n    def _validate_prophet_model(\n        self,\n        model: ProphetForecaster,\n        test_data: pd.DataFrame\n    ) -> Dict:\n        \"\"\"Validate Prophet model on test data.\"\"\"\n        \n        try:\n            # Make predictions\n            test_prepared = model.prepare_data(test_data, target_col='price')\n            forecast = model.predict(test_prepared, include_history=False)\n            \n            # Align predictions with actual values\n            actual_values = test_prepared['y'].values\n            predicted_values = forecast['yhat'].values[:len(actual_values)]\n            \n            # Calculate metrics\n            mae = mean_absolute_error(actual_values, predicted_values)\n            rmse = np.sqrt(mean_squared_error(actual_values, predicted_values))\n            mape = np.mean(np.abs((actual_values - predicted_values) / actual_values)) * 100\n            r2 = r2_score(actual_values, predicted_values)\n            \n            return {\n                'mae': float(mae),\n                'rmse': float(rmse),\n                'mape': float(mape),\n                'r2': float(r2)\n            }\n            \n        except Exception as e:\n            ml_logger.warning(\"Prophet validation failed\", error=str(e))\n            return {}\n    \n    def cross_validate_models(\n        self,\n        commodity_code: str,\n        data: pd.DataFrame,\n        model_types: List[str] = ['prophet', 'lstm'],\n        n_splits: int = 5\n    ) -> Dict:\n        \"\"\"Perform cross-validation on models.\"\"\"\n        \n        try:\n            # Time series cross-validation\n            tscv = TimeSeriesSplit(n_splits=n_splits)\n            \n            cv_results = {}\n            \n            for model_type in model_types:\n                model_scores = []\n                \n                for fold, (train_idx, test_idx) in enumerate(tscv.split(data)):\n                    train_fold = data.iloc[train_idx]\n                    test_fold = data.iloc[test_idx]\n                    \n                    try:\n                        if model_type == 'prophet':\n                            model = ProphetForecaster(\n                                commodity_code=commodity_code\n                            )\n                            model.fit(train_fold, target_col='price')\n                            metrics = self._validate_prophet_model(model, test_fold)\n                        \n                        elif model_type == 'lstm':\n                            model = LSTMForecaster(\n                                commodity_code=commodity_code\n                            )\n                            feature_cols = self.feature_engineer.get_feature_importance_names(train_fold)\n                            model.fit(\n                                train_fold,\n                                target_col='price',\n                                feature_cols=feature_cols,\n                                verbose=0\n                            )\n                            metrics = model.evaluate_model(test_fold, target_col='price')\n                        \n                        model_scores.append(metrics)\n                        \n                    except Exception as e:\n                        ml_logger.warning(\n                            f\"Cross-validation fold {fold} failed for {model_type}\",\n                            error=str(e)\n                        )\n                \n                # Calculate average metrics\n                if model_scores:\n                    avg_metrics = {}\n                    for metric in model_scores[0].keys():\n                        values = [score[metric] for score in model_scores if metric in score]\n                        if values:\n                            avg_metrics[f\"{metric}_mean\"] = np.mean(values)\n                            avg_metrics[f\"{metric}_std\"] = np.std(values)\n                    \n                    cv_results[model_type] = avg_metrics\n            \n            ml_logger.info(\n                \"Cross-validation completed\",\n                commodity_code=commodity_code,\n                results=cv_results\n            )\n            \n            return cv_results\n            \n        except Exception as e:\n            ml_logger.error(\"Cross-validation failed\", error=str(e))\n            raise e\n    \n    async def _cache_features(self, commodity_code: str, features_df: pd.DataFrame):\n        \"\"\"Cache engineered features.\"\"\"\n        \n        try:\n            feature_summary = self.feature_engineer.create_feature_summary(features_df)\n            \n            # Store in Redis\n            cache_key = f\"features:{commodity_code}\"\n            await redis_manager.store_features(\n                cache_key,\n                {\n                    'features_summary': feature_summary,\n                    'feature_columns': self.feature_engineer.get_feature_importance_names(features_df),\n                    'data_shape': features_df.shape,\n                    'cache_date': datetime.now().isoformat()\n                }\n            )\n            \n            # Save to file system\n            features_path = os.path.join(\n                settings.feature_store_path,\n                f\"{commodity_code}_features.parquet\"\n            )\n            features_df.to_parquet(features_path)\n            \n            ml_logger.info(\n                \"Features cached\",\n                commodity_code=commodity_code,\n                cache_key=cache_key,\n                features_path=features_path\n            )\n            \n        except Exception as e:\n            ml_logger.warning(\"Failed to cache features\", error=str(e))\n    \n    def _summarize_training_results(self, results: Dict) -> Dict:\n        \"\"\"Summarize training results for logging.\"\"\"\n        \n        summary = {\n            'total_models_trained': 0,\n            'successful_models': 0,\n            'failed_models': 0,\n            'skipped_models': 0,\n            'model_types': set(),\n            'regions': list(results.keys())\n        }\n        \n        for region, region_results in results.items():\n            for model_type, model_result in region_results.items():\n                summary['total_models_trained'] += 1\n                summary['model_types'].add(model_type)\n                \n                status = model_result.get('status', 'unknown')\n                if status == 'success':\n                    summary['successful_models'] += 1\n                elif status == 'skipped':\n                    summary['skipped_models'] += 1\n                else:\n                    summary['failed_models'] += 1\n        \n        summary['model_types'] = list(summary['model_types'])\n        \n        return summary\n    \n    def get_training_status(self, commodity_code: str = None) -> Dict:\n        \"\"\"Get training status for commodities.\"\"\"\n        \n        if commodity_code:\n            return self.training_results.get(commodity_code, {})\n        else:\n            return self.training_results\n    \n    def load_model(self, model_type: str, commodity_code: str, region_code: str = None):\n        \"\"\"Load trained model.\"\"\"\n        \n        model_key = f\"{model_type}_{commodity_code}_{region_code or 'national'}\"\n        \n        # Check if already loaded in memory\n        if model_key in self.models:\n            return self.models[model_key]\n        \n        # Load from file\n        model_path = os.path.join(settings.model_store_path, f\"{model_key}.pkl\")\n        \n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Model not found: {model_path}\")\n        \n        try:\n            if model_type == 'prophet':\n                model = ProphetForecaster(commodity_code, region_code)\n                model.load_model(model_path)\n            elif model_type == 'lstm':\n                model = LSTMForecaster(commodity_code, region_code)\n                model.load_model(model_path)\n            elif model_type == 'anomaly':\n                with open(model_path, 'rb') as f:\n                    model = pickle.load(f)\n            else:\n                raise ValueError(f\"Unknown model type: {model_type}\")\n            \n            # Cache in memory\n            self.models[model_key] = model\n            \n            return model\n            \n        except Exception as e:\n            ml_logger.error(\"Failed to load model\", model_path=model_path, error=str(e))\n            raise e\n    \n    def cleanup_old_models(self, days_threshold: int = 30):\n        \"\"\"Clean up old model files.\"\"\"\n        \n        try:\n            model_dir = Path(settings.model_store_path)\n            cutoff_date = datetime.now() - timedelta(days=days_threshold)\n            \n            removed_count = 0\n            \n            for model_file in model_dir.glob(\"*.pkl\"):\n                if model_file.stat().st_mtime < cutoff_date.timestamp():\n                    model_file.unlink()\n                    removed_count += 1\n            \n            ml_logger.info(\n                \"Model cleanup completed\",\n                removed_models=removed_count,\n                days_threshold=days_threshold\n            )\n            \n        except Exception as e:\n            ml_logger.error(\"Model cleanup failed\", error=str(e))\n