"""Anomaly detection models untuk price monitoring."""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Union
import pickle
import warnings

try:
    from sklearn.cluster import DBSCAN
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler, RobustScaler
    from sklearn.decomposition import PCA
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn.metrics import silhouette_score, calinski_harabasz_score
    from scipy import stats
    from scipy.spatial.distance import pdist, squareform
except ImportError:
    print("Scikit-learn not installed. Please install with: pip install scikit-learn")

from app.config.logging import ml_logger
from app.config.settings import settings

warnings.filterwarnings('ignore')


class AnomalyDetector:
    """Combined anomaly detection using multiple algorithms."""
    
    def __init__(
        self,
        commodity_code: str = None,
        region_code: str = None
    ):
        self.commodity_code = commodity_code
        self.region_code = region_code
        
        # Models
        self.temporal_detector = TemporalAnomalyDetector()
        self.geographic_detector = GeographicAnomalyDetector()
        
        # Scalers
        self.scaler = RobustScaler()
        self.is_fitted = False
        
        # Detection results
        self.detection_results = {}
        self.feature_columns = []
        
    def fit(
        self,
        df: pd.DataFrame,
        detection_types: List[str] = ['temporal', 'geographic']
    ) -> Dict:
        """Fit anomaly detection models."""
        
        try:
            results = {}\n            \n            # Temporal anomaly detection\n            if 'temporal' in detection_types:\n                temporal_results = self.temporal_detector.fit(df)\n                results['temporal'] = temporal_results\n            \n            # Geographic anomaly detection\n            if 'geographic' in detection_types:\n                geographic_results = self.geographic_detector.fit(df)\n                results['geographic'] = geographic_results\n            \n            self.is_fitted = True\n            self.detection_results = results\n            \n            ml_logger.info(\n                \"Anomaly detection models fitted\",\n                commodity_code=self.commodity_code,\n                detection_types=detection_types,\n                results=results\n            )\n            \n            return results\n            \n        except Exception as e:\n            ml_logger.error(\n                \"Failed to fit anomaly detection models\",\n                commodity_code=self.commodity_code,\n                error=str(e)\n            )\n            raise e\n    \n    def detect_anomalies(\n        self,\n        df: pd.DataFrame,\n        detection_types: List[str] = ['temporal', 'geographic'],\n        sensitivity: float = 0.1\n    ) -> pd.DataFrame:\n        \"\"\"Detect anomalies in price data.\"\"\"\n        \n        if not self.is_fitted:\n            # Fit models if not already fitted\n            self.fit(df, detection_types)\n        \n        try:\n            anomalies = []\n            \n            # Temporal anomaly detection\n            if 'temporal' in detection_types:\n                temporal_anomalies = self.temporal_detector.detect_anomalies(\n                    df, sensitivity=sensitivity\n                )\n                anomalies.extend(temporal_anomalies)\n            \n            # Geographic anomaly detection\n            if 'geographic' in detection_types:\n                geographic_anomalies = self.geographic_detector.detect_anomalies(\n                    df, sensitivity=sensitivity\n                )\n                anomalies.extend(geographic_anomalies)\n            \n            # Convert to DataFrame\n            if anomalies:\n                anomalies_df = pd.DataFrame(anomalies)\n                # Remove duplicates based on date, commodity, and region\n                anomalies_df = anomalies_df.drop_duplicates(\n                    subset=['date', 'commodity_code', 'region_code']\n                )\n            else:\n                anomalies_df = pd.DataFrame()\n            \n            ml_logger.info(\n                \"Anomaly detection completed\",\n                commodity_code=self.commodity_code,\n                total_anomalies=len(anomalies_df),\n                detection_types=detection_types\n            )\n            \n            return anomalies_df\n            \n        except Exception as e:\n            ml_logger.error(\n                \"Anomaly detection failed\",\n                commodity_code=self.commodity_code,\n                error=str(e)\n            )\n            raise e\n\n\nclass TemporalAnomalyDetector:\n    \"\"\"Temporal anomaly detection using Isolation Forest.\"\"\"\n    \n    def __init__(\n        self,\n        contamination: float = None,\n        n_estimators: int = None\n    ):\n        self.contamination = contamination or settings.isolation_forest_contamination\n        self.n_estimators = n_estimators or settings.isolation_forest_n_estimators\n        \n        self.isolation_forest = IsolationForest(\n            contamination=self.contamination,\n            n_estimators=self.n_estimators,\n            random_state=42,\n            n_jobs=-1\n        )\n        \n        self.scaler = StandardScaler()\n        self.feature_columns = []\n        self.is_fitted = False\n        self.detection_metadata = {}\n        \n    def _prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Prepare features for temporal anomaly detection.\"\"\"\n        \n        features_df = df.copy()\n        \n        # Ensure we have required columns\n        if 'price' not in features_df.columns:\n            raise ValueError(\"Price column is required\")\n        \n        # Sort by commodity, region, and date\n        if 'date' in features_df.columns:\n            features_df = features_df.sort_values(['commodity_code', 'region_code', 'date'])\n        \n        # Group by commodity and region to calculate features\n        processed_groups = []\n        \n        for (commodity, region), group in features_df.groupby(['commodity_code', 'region_code']):\n            group = group.copy()\n            \n            # Price-based features\n            group['price_zscore'] = stats.zscore(group['price'])\n            group['price_pct_change'] = group['price'].pct_change()\n            group['price_log'] = np.log(group['price'])\n            group['price_log_change'] = group['price_log'].diff()\n            \n            # Rolling statistics\n            for window in [7, 14, 30]:\n                group[f'price_ma_{window}'] = group['price'].rolling(window).mean()\n                group[f'price_std_{window}'] = group['price'].rolling(window).std()\n                group[f'price_zscore_{window}'] = (\n                    (group['price'] - group[f'price_ma_{window}']) / \n                    group[f'price_std_{window}']\n                )\n            \n            # Volatility features\n            group['volatility_7d'] = group['price_pct_change'].rolling(7).std()\n            group['volatility_30d'] = group['price_pct_change'].rolling(30).std()\n            \n            # Trend features\n            group['price_trend_7d'] = group['price'].rolling(7).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 7 else np.nan\n            )\n            group['price_trend_30d'] = group['price'].rolling(30).apply(\n                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 30 else np.nan\n            )\n            \n            # Price momentum\n            group['momentum_3d'] = group['price'].diff(3)\n            group['momentum_7d'] = group['price'].diff(7)\n            \n            # Relative position in recent range\n            for window in [14, 30]:\n                min_price = group['price'].rolling(window).min()\n                max_price = group['price'].rolling(window).max()\n                group[f'price_position_{window}d'] = (\n                    (group['price'] - min_price) / (max_price - min_price)\n                )\n            \n            processed_groups.append(group)\n        \n        return pd.concat(processed_groups, ignore_index=True)\n    \n    def fit(self, df: pd.DataFrame) -> Dict:\n        \"\"\"Fit temporal anomaly detection model.\"\"\"\n        \n        try:\n            # Prepare features\n            features_df = self._prepare_features(df)\n            \n            # Select feature columns\n            feature_cols = [\n                col for col in features_df.columns\n                if any(x in col for x in [\n                    'zscore', 'pct_change', 'volatility', 'trend',\n                    'momentum', 'position', 'ma_', 'std_'\n                ])\n            ]\n            \n            # Remove rows with NaN values\n            features_clean = features_df[feature_cols].dropna()\n            \n            if len(features_clean) < 50:\n                raise ValueError(\"Not enough clean data for temporal anomaly detection\")\n            \n            # Scale features\n            X_scaled = self.scaler.fit_transform(features_clean)\n            \n            # Fit Isolation Forest\n            self.isolation_forest.fit(X_scaled)\n            \n            # Store feature columns\n            self.feature_columns = feature_cols\n            self.is_fitted = True\n            \n            # Calculate fitting metrics\n            anomaly_scores = self.isolation_forest.decision_function(X_scaled)\n            outlier_labels = self.isolation_forest.predict(X_scaled)\n            \n            n_outliers = np.sum(outlier_labels == -1)\n            outlier_percentage = (n_outliers / len(X_scaled)) * 100\n            \n            self.detection_metadata = {\n                'model_type': 'temporal_isolation_forest',\n                'training_samples': len(X_scaled),\n                'feature_count': len(feature_cols),\n                'contamination': self.contamination,\n                'n_estimators': self.n_estimators,\n                'detected_outliers': int(n_outliers),\n                'outlier_percentage': float(outlier_percentage),\n                'anomaly_score_stats': {\n                    'mean': float(np.mean(anomaly_scores)),\n                    'std': float(np.std(anomaly_scores)),\n                    'min': float(np.min(anomaly_scores)),\n                    'max': float(np.max(anomaly_scores))\n                }\n            }\n            \n            ml_logger.info(\n                \"Temporal anomaly detector fitted\",\n                **self.detection_metadata\n            )\n            \n            return self.detection_metadata\n            \n        except Exception as e:\n            ml_logger.error(\"Failed to fit temporal anomaly detector\", error=str(e))\n            raise e\n    \n    def detect_anomalies(\n        self,\n        df: pd.DataFrame,\n        sensitivity: float = 0.1\n    ) -> List[Dict]:\n        \"\"\"Detect temporal anomalies.\"\"\"\n        \n        if not self.is_fitted:\n            raise ValueError(\"Model must be fitted before detecting anomalies\")\n        \n        try:\n            # Prepare features\n            features_df = self._prepare_features(df)\n            \n            # Get features for rows with complete data\n            complete_mask = features_df[self.feature_columns].notna().all(axis=1)\n            features_complete = features_df[complete_mask].copy()\n            \n            if len(features_complete) == 0:\n                return []\n            \n            # Scale features\n            X_scaled = self.scaler.transform(features_complete[self.feature_columns])\n            \n            # Get anomaly scores\n            anomaly_scores = self.isolation_forest.decision_function(X_scaled)\n            outlier_labels = self.isolation_forest.predict(X_scaled)\n            \n            # Adjust threshold based on sensitivity\n            score_threshold = np.percentile(anomaly_scores, sensitivity * 100)\n            \n            anomalies = []\n            \n            for idx, (_, row) in enumerate(features_complete.iterrows()):\n                anomaly_score = anomaly_scores[idx]\n                is_outlier = outlier_labels[idx] == -1\n                is_sensitive_outlier = anomaly_score <= score_threshold\n                \n                if is_outlier or is_sensitive_outlier:\n                    # Determine anomaly type and severity\n                    anomaly_type, severity = self._classify_temporal_anomaly(\n                        row, anomaly_score\n                    )\n                    \n                    anomaly = {\n                        'date': row['date'] if 'date' in row else datetime.now(),\n                        'commodity_code': row.get('commodity_code', ''),\n                        'region_code': row.get('region_code', ''),\n                        'actual_price': float(row['price']),\n                        'expected_price': self._calculate_expected_price(row),\n                        'anomaly_score': float(abs(anomaly_score)),\n                        'anomaly_type': anomaly_type,\n                        'severity': severity,\n                        'explanation': self._explain_temporal_anomaly(row, anomaly_score),\n                        'detection_method': 'temporal_isolation_forest'\n                    }\n                    \n                    anomalies.append(anomaly)\n            \n            return anomalies\n            \n        except Exception as e:\n            ml_logger.error(\"Temporal anomaly detection failed\", error=str(e))\n            raise e\n    \n    def _classify_temporal_anomaly(\n        self,\n        row: pd.Series,\n        anomaly_score: float\n    ) -> Tuple[str, str]:\n        \"\"\"Classify temporal anomaly type and severity.\"\"\"\n        \n        # Determine anomaly type based on price changes\n        price_change = row.get('price_pct_change', 0)\n        \n        if price_change > 0.1:  # 10% increase\n            anomaly_type = 'price_spike'\n        elif price_change < -0.1:  # 10% decrease\n            anomaly_type = 'price_drop'\n        else:\n            anomaly_type = 'volatility_anomaly'\n        \n        # Determine severity based on anomaly score\n        score_abs = abs(anomaly_score)\n        if score_abs > 0.8:\n            severity = 'critical'\n        elif score_abs > 0.6:\n            severity = 'high'\n        elif score_abs > 0.4:\n            severity = 'medium'\n        else:\n            severity = 'low'\n        \n        return anomaly_type, severity\n    \n    def _calculate_expected_price(self, row: pd.Series) -> float:\n        \"\"\"Calculate expected price based on moving average.\"\"\"\n        \n        # Use 30-day moving average as expected price\n        expected = row.get('price_ma_30', row.get('price', 0))\n        \n        if pd.isna(expected):\n            expected = row.get('price', 0)\n        \n        return float(expected)\n    \n    def _explain_temporal_anomaly(self, row: pd.Series, anomaly_score: float) -> str:\n        \"\"\"Generate explanation for temporal anomaly.\"\"\"\n        \n        price_change = row.get('price_pct_change', 0) * 100\n        volatility = row.get('volatility_7d', 0) * 100\n        \n        explanation_parts = []\n        \n        if abs(price_change) > 10:\n            direction = \"naik\" if price_change > 0 else \"turun\"\n            explanation_parts.append(f\"Harga {direction} {abs(price_change):.1f}% dalam sehari\")\n        \n        if volatility > 5:\n            explanation_parts.append(f\"Volatilitas tinggi ({volatility:.1f}%) dalam 7 hari terakhir\")\n        \n        if abs(anomaly_score) > 0.6:\n            explanation_parts.append(\"Pola harga sangat tidak biasa dibanding historical\")\n        \n        if not explanation_parts:\n            explanation_parts.append(\"Anomali terdeteksi dalam pola temporal harga\")\n        \n        return \"; \".join(explanation_parts)\n\n\nclass GeographicAnomalyDetector:\n    \"\"\"Geographic anomaly detection using DBSCAN clustering.\"\"\"\n    \n    def __init__(\n        self,\n        eps: float = None,\n        min_samples: int = None\n    ):\n        self.eps = eps or settings.dbscan_eps\n        self.min_samples = min_samples or settings.dbscan_min_samples\n        \n        self.dbscan = DBSCAN(\n            eps=self.eps,\n            min_samples=self.min_samples,\n            metric='euclidean'\n        )\n        \n        self.scaler = StandardScaler()\n        self.feature_columns = []\n        self.is_fitted = False\n        self.detection_metadata = {}\n        \n    def _prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Prepare features for geographic anomaly detection.\"\"\"\n        \n        features_df = df.copy()\n        \n        # Ensure we have required columns\n        required_cols = ['price', 'commodity_code', 'region_code']\n        for col in required_cols:\n            if col not in features_df.columns:\n                raise ValueError(f\"Required column '{col}' not found\")\n        \n        # Group by date and commodity to compare across regions\n        processed_groups = []\n        \n        for (date, commodity), group in features_df.groupby(['date', 'commodity_code']):\n            if len(group) < 2:  # Need at least 2 regions for comparison\n                continue\n            \n            group = group.copy()\n            \n            # Regional price statistics\n            mean_price = group['price'].mean()\n            std_price = group['price'].std()\n            \n            group['price_vs_national_mean'] = group['price'] - mean_price\n            group['price_zscore_regional'] = (\n                (group['price'] - mean_price) / std_price\n            ) if std_price > 0 else 0\n            \n            # Price rank within the date-commodity group\n            group['regional_price_rank'] = group['price'].rank(ascending=False)\n            group['regional_price_percentile'] = group['price'].rank(pct=True)\n            \n            # Distance from median\n            median_price = group['price'].median()\n            group['price_vs_median'] = group['price'] - median_price\n            \n            # Relative price position\n            min_price = group['price'].min()\n            max_price = group['price'].max()\n            price_range = max_price - min_price\n            \n            if price_range > 0:\n                group['price_position_in_range'] = (\n                    (group['price'] - min_price) / price_range\n                )\n            else:\n                group['price_position_in_range'] = 0.5\n            \n            processed_groups.append(group)\n        \n        if not processed_groups:\n            return pd.DataFrame()\n        \n        return pd.concat(processed_groups, ignore_index=True)\n    \n    def fit(self, df: pd.DataFrame) -> Dict:\n        \"\"\"Fit geographic anomaly detection model.\"\"\"\n        \n        try:\n            # Prepare features\n            features_df = self._prepare_features(df)\n            \n            if features_df.empty:\n                raise ValueError(\"No valid data for geographic anomaly detection\")\n            \n            # Select feature columns\n            feature_cols = [\n                'price_vs_national_mean', 'price_zscore_regional',\n                'regional_price_rank', 'regional_price_percentile',\n                'price_vs_median', 'price_position_in_range'\n            ]\n            \n            # Filter for existing columns\n            feature_cols = [col for col in feature_cols if col in features_df.columns]\n            \n            # Remove rows with NaN values\n            features_clean = features_df[feature_cols].dropna()\n            \n            if len(features_clean) < 10:\n                raise ValueError(\"Not enough clean data for geographic anomaly detection\")\n            \n            # Scale features\n            X_scaled = self.scaler.fit_transform(features_clean)\n            \n            # Fit DBSCAN\n            cluster_labels = self.dbscan.fit_predict(X_scaled)\n            \n            # Store feature columns\n            self.feature_columns = feature_cols\n            self.is_fitted = True\n            \n            # Calculate fitting metrics\n            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n            n_outliers = np.sum(cluster_labels == -1)\n            outlier_percentage = (n_outliers / len(cluster_labels)) * 100\n            \n            # Calculate silhouette score if we have clusters\n            silhouette = 0\n            if n_clusters > 1 and n_outliers < len(cluster_labels):\n                try:\n                    silhouette = silhouette_score(X_scaled, cluster_labels)\n                except:\n                    pass\n            \n            self.detection_metadata = {\n                'model_type': 'geographic_dbscan',\n                'training_samples': len(X_scaled),\n                'feature_count': len(feature_cols),\n                'eps': self.eps,\n                'min_samples': self.min_samples,\n                'n_clusters': int(n_clusters),\n                'detected_outliers': int(n_outliers),\n                'outlier_percentage': float(outlier_percentage),\n                'silhouette_score': float(silhouette)\n            }\n            \n            ml_logger.info(\n                \"Geographic anomaly detector fitted\",\n                **self.detection_metadata\n            )\n            \n            return self.detection_metadata\n            \n        except Exception as e:\n            ml_logger.error(\"Failed to fit geographic anomaly detector\", error=str(e))\n            raise e\n    \n    def detect_anomalies(\n        self,\n        df: pd.DataFrame,\n        sensitivity: float = 0.1\n    ) -> List[Dict]:\n        \"\"\"Detect geographic anomalies.\"\"\"\n        \n        if not self.is_fitted:\n            raise ValueError(\"Model must be fitted before detecting anomalies\")\n        \n        try:\n            # Prepare features\n            features_df = self._prepare_features(df)\n            \n            if features_df.empty:\n                return []\n            \n            # Get features for rows with complete data\n            complete_mask = features_df[self.feature_columns].notna().all(axis=1)\n            features_complete = features_df[complete_mask].copy()\n            \n            if len(features_complete) == 0:\n                return []\n            \n            # Scale features\n            X_scaled = self.scaler.transform(features_complete[self.feature_columns])\n            \n            # Get cluster labels\n            cluster_labels = self.dbscan.fit_predict(X_scaled)\n            \n            anomalies = []\n            \n            for idx, (_, row) in enumerate(features_complete.iterrows()):\n                cluster_label = cluster_labels[idx]\n                \n                # Outliers have cluster label -1\n                if cluster_label == -1:\n                    # Calculate anomaly score based on distance to nearest cluster\n                    anomaly_score = self._calculate_geographic_anomaly_score(\n                        X_scaled[idx], X_scaled, cluster_labels\n                    )\n                    \n                    # Apply sensitivity threshold\n                    if anomaly_score >= sensitivity:\n                        # Determine anomaly type and severity\n                        anomaly_type, severity = self._classify_geographic_anomaly(\n                            row, anomaly_score\n                        )\n                        \n                        anomaly = {\n                            'date': row['date'] if 'date' in row else datetime.now(),\n                            'commodity_code': row.get('commodity_code', ''),\n                            'region_code': row.get('region_code', ''),\n                            'actual_price': float(row['price']),\n                            'expected_price': self._calculate_expected_price_geographic(row),\n                            'anomaly_score': float(anomaly_score),\n                            'anomaly_type': anomaly_type,\n                            'severity': severity,\n                            'explanation': self._explain_geographic_anomaly(row, anomaly_score),\n                            'detection_method': 'geographic_dbscan'\n                        }\n                        \n                        anomalies.append(anomaly)\n            \n            return anomalies\n            \n        except Exception as e:\n            ml_logger.error(\"Geographic anomaly detection failed\", error=str(e))\n            raise e\n    \n    def _calculate_geographic_anomaly_score(\n        self,\n        point: np.ndarray,\n        X: np.ndarray,\n        labels: np.ndarray\n    ) -> float:\n        \"\"\"Calculate anomaly score for geographic outlier.\"\"\"\n        \n        # Find points that belong to clusters (not outliers)\n        cluster_points = X[labels != -1]\n        \n        if len(cluster_points) == 0:\n            return 1.0  # Maximum anomaly score\n        \n        # Calculate distance to nearest cluster point\n        distances = np.linalg.norm(cluster_points - point, axis=1)\n        min_distance = np.min(distances)\n        \n        # Normalize distance to 0-1 range\n        max_distance = np.max(pdist(X)) if len(X) > 1 else 1\n        normalized_score = min_distance / max_distance\n        \n        return min(normalized_score, 1.0)\n    \n    def _classify_geographic_anomaly(\n        self,\n        row: pd.Series,\n        anomaly_score: float\n    ) -> Tuple[str, str]:\n        \"\"\"Classify geographic anomaly type and severity.\"\"\"\n        \n        # Determine anomaly type based on price position\n        price_percentile = row.get('regional_price_percentile', 0.5)\n        \n        if price_percentile > 0.8:\n            anomaly_type = 'geographic_price_spike'\n        elif price_percentile < 0.2:\n            anomaly_type = 'geographic_price_drop'\n        else:\n            anomaly_type = 'geographic_outlier'\n        \n        # Determine severity based on anomaly score\n        if anomaly_score > 0.8:\n            severity = 'critical'\n        elif anomaly_score > 0.6:\n            severity = 'high'\n        elif anomaly_score > 0.4:\n            severity = 'medium'\n        else:\n            severity = 'low'\n        \n        return anomaly_type, severity\n    \n    def _calculate_expected_price_geographic(self, row: pd.Series) -> float:\n        \"\"\"Calculate expected price based on regional comparison.\"\"\"\n        \n        # Use regional mean as expected price\n        price_vs_mean = row.get('price_vs_national_mean', 0)\n        actual_price = row.get('price', 0)\n        \n        expected = actual_price - price_vs_mean\n        \n        return float(expected)\n    \n    def _explain_geographic_anomaly(self, row: pd.Series, anomaly_score: float) -> str:\n        \"\"\"Generate explanation for geographic anomaly.\"\"\"\n        \n        price_percentile = row.get('regional_price_percentile', 0.5) * 100\n        price_vs_mean = row.get('price_vs_national_mean', 0)\n        \n        explanation_parts = []\n        \n        if price_percentile > 80:\n            explanation_parts.append(f\"Harga tertinggi di antara region ({price_percentile:.0f}th percentile)\")\n        elif price_percentile < 20:\n            explanation_parts.append(f\"Harga terendah di antara region ({price_percentile:.0f}th percentile)\")\n        \n        if abs(price_vs_mean) > 1000:\n            direction = \"di atas\" if price_vs_mean > 0 else \"di bawah\"\n            explanation_parts.append(f\"Harga {direction} rata-rata nasional Rp {abs(price_vs_mean):,.0f}\")\n        \n        if anomaly_score > 0.7:\n            explanation_parts.append(\"Pola harga sangat berbeda dari region lain\")\n        \n        if not explanation_parts:\n            explanation_parts.append(\"Anomali geografis terdeteksi dalam perbandingan antar region\")\n        \n        return \"; \".join(explanation_parts)\n    \n    def save_model(self, filepath: str):\n        \"\"\"Save anomaly detection model.\"\"\"\n        \n        model_data = {\n            'dbscan': self.dbscan,\n            'scaler': self.scaler,\n            'feature_columns': self.feature_columns,\n            'is_fitted': self.is_fitted,\n            'detection_metadata': self.detection_metadata,\n            'eps': self.eps,\n            'min_samples': self.min_samples\n        }\n        \n        with open(filepath, 'wb') as f:\n            pickle.dump(model_data, f)\n    \n    def load_model(self, filepath: str):\n        \"\"\"Load anomaly detection model.\"\"\"\n        \n        with open(filepath, 'rb') as f:\n            model_data = pickle.load(f)\n        \n        self.dbscan = model_data['dbscan']\n        self.scaler = model_data['scaler']\n        self.feature_columns = model_data['feature_columns']\n        self.is_fitted = model_data['is_fitted']\n        self.detection_metadata = model_data['detection_metadata']\n        self.eps = model_data['eps']\n        self.min_samples = model_data['min_samples']\n