"""Inference pipeline untuk real-time predictions."""

import asyncio
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union, Any
import numpy as np
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings

from app.config.database import db_manager
from app.config.logging import ml_logger
from app.config.settings import settings
from app.config.redis import redis_manager
from app.preprocessing.data_processor import DataProcessor
from app.features.engineering import FeatureEngineer
from app.training.trainer import ModelTrainer

warnings.filterwarnings('ignore')


class PredictionService:
    """Service untuk melakukan prediksi harga komoditas."""
    
    def __init__(self):
        self.data_processor = DataProcessor()
        self.feature_engineer = FeatureEngineer()
        self.model_trainer = ModelTrainer()
        self.prediction_cache = {}
        self.model_cache = {}
        
        # Performance tracking
        self.prediction_stats = {
            'total_predictions': 0,
            'cache_hits': 0,
            'average_latency_ms': 0,
            'last_prediction_time': None
        }
    
    async def predict_price(\n        self,\n        commodity_code: str,\n        region_code: str = None,\n        horizon_days: int = 7,\n        model_type: str = 'prophet',\n        include_uncertainty: bool = True,\n        include_features: bool = False\n    ) -> Dict:\n        \"\"\"Predict commodity price.\"\"\"\n        \n        start_time = time.time()\n        \n        try:\n            # Generate cache key\n            cache_key = self._generate_cache_key(\n                commodity_code, region_code, horizon_days, model_type\n            )\n            \n            # Check cache first\n            cached_prediction = await redis_manager.get_cached_predictions(cache_key)\n            if cached_prediction:\n                self.prediction_stats['cache_hits'] += 1\n                ml_logger.info(\n                    \"Prediction served from cache\",\n                    commodity_code=commodity_code,\n                    cache_key=cache_key\n                )\n                return cached_prediction\n            \n            # Load model\n            model = await self._load_model(model_type, commodity_code, region_code)\n            \n            # Get recent data for prediction\n            recent_data = await self._get_recent_data(\n                commodity_code, region_code, lookback_days=90\n            )\n            \n            # Make prediction\n            prediction_result = await self._make_prediction(\n                model=model,\n                data=recent_data,\n                commodity_code=commodity_code,\n                region_code=region_code,\n                horizon_days=horizon_days,\n                model_type=model_type,\n                include_uncertainty=include_uncertainty,\n                include_features=include_features\n            )\n            \n            # Cache prediction\n            await redis_manager.cache_model_predictions(\n                cache_key, prediction_result, expire=1800  # 30 minutes\n            )\n            \n            # Update stats\n            latency_ms = (time.time() - start_time) * 1000\n            self._update_prediction_stats(latency_ms)\n            \n            # Log prediction\n            ml_logger.log_prediction_request(\n                request_id=cache_key,\n                commodity_code=commodity_code,\n                region_code=region_code,\n                prediction_horizon=horizon_days,\n                latency_ms=latency_ms\n            )\n            \n            return prediction_result\n            \n        except Exception as e:\n            ml_logger.error(\n                \"Price prediction failed\",\n                commodity_code=commodity_code,\n                region_code=region_code,\n                model_type=model_type,\n                error=str(e)\n            )\n            raise e\n    \n    async def batch_predict(\n        self,\n        requests: List[Dict],\n        max_workers: int = 5\n    ) -> List[Dict]:\n        \"\"\"Perform batch predictions.\"\"\"\n        \n        start_time = time.time()\n        \n        try:\n            ml_logger.info(\n                \"Starting batch prediction\",\n                request_count=len(requests),\n                max_workers=max_workers\n            )\n            \n            results = []\n            \n            # Use ThreadPoolExecutor for concurrent predictions\n            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n                # Create async tasks\n                future_to_request = {\n                    executor.submit(self._predict_single_async, req): req\n                    for req in requests\n                }\n                \n                # Collect results\n                for future in as_completed(future_to_request):\n                    request = future_to_request[future]\n                    try:\n                        result = future.result()\n                        results.append({\n                            'status': 'success',\n                            'request': request,\n                            'prediction': result\n                        })\n                    except Exception as e:\n                        ml_logger.error(\n                            \"Batch prediction item failed\",\n                            request=request,\n                            error=str(e)\n                        )\n                        results.append({\n                            'status': 'error',\n                            'request': request,\n                            'error': str(e)\n                        })\n            \n            # Calculate statistics\n            successful = sum(1 for r in results if r['status'] == 'success')\n            failed = len(results) - successful\n            processing_time_ms = (time.time() - start_time) * 1000\n            \n            batch_result = {\n                'total_requests': len(requests),\n                'successful_predictions': successful,\n                'failed_predictions': failed,\n                'processing_time_ms': processing_time_ms,\n                'predictions': results\n            }\n            \n            ml_logger.info(\n                \"Batch prediction completed\",\n                **{k: v for k, v in batch_result.items() if k != 'predictions'}\n            )\n            \n            return batch_result\n            \n        except Exception as e:\n            ml_logger.error(\"Batch prediction failed\", error=str(e))\n            raise e\n    \n    def _predict_single_async(self, request: Dict) -> Dict:\n        \"\"\"Wrapper for async prediction in thread pool.\"\"\"\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        try:\n            return loop.run_until_complete(self.predict_price(**request))\n        finally:\n            loop.close()\n    \n    async def detect_anomalies(\n        self,\n        commodity_code: str = None,\n        region_code: str = None,\n        detection_type: str = 'both',\n        sensitivity: float = 0.1,\n        time_window_days: int = 30\n    ) -> Dict:\n        \"\"\"Detect price anomalies.\"\"\"\n        \n        try:\n            # Generate cache key for anomaly detection\n            cache_key = f\"anomaly:{commodity_code or 'all'}:{region_code or 'all'}:{detection_type}:{sensitivity}\"\n            \n            # Check cache\n            cached_anomalies = await redis_manager.get_cached_anomaly_scores(cache_key)\n            if cached_anomalies:\n                return cached_anomalies\n            \n            # Load anomaly detection model\n            model = await self._load_model('anomaly', commodity_code)\n            \n            # Get recent data\n            recent_data = await self._get_recent_data(\n                commodity_code, region_code, lookback_days=time_window_days\n            )\n            \n            # Detect anomalies\n            detection_types = [\n                detection_type\n            ] if detection_type != 'both' else ['temporal', 'geographic']\n            \n            anomalies_df = model.detect_anomalies(\n                recent_data,\n                detection_types=detection_types,\n                sensitivity=sensitivity\n            )\n            \n            # Process results\n            anomalies_list = []\n            if not anomalies_df.empty:\n                for _, anomaly in anomalies_df.iterrows():\n                    anomaly_dict = anomaly.to_dict()\n                    # Convert datetime to string for JSON serialization\n                    if 'date' in anomaly_dict and hasattr(anomaly_dict['date'], 'isoformat'):\n                        anomaly_dict['date'] = anomaly_dict['date'].isoformat()\n                    anomalies_list.append(anomaly_dict)\n            \n            # Calculate summary statistics\n            summary_stats = self._calculate_anomaly_summary(anomalies_df)\n            \n            result = {\n                'detection_type': detection_type,\n                'total_anomalies': len(anomalies_list),\n                'anomalies': anomalies_list,\n                'summary_statistics': summary_stats,\n                'detection_metadata': {\n                    'commodity_code': commodity_code,\n                    'region_code': region_code,\n                    'sensitivity': sensitivity,\n                    'time_window_days': time_window_days,\n                    'detection_date': datetime.now().isoformat()\n                }\n            }\n            \n            # Cache results\n            await redis_manager.cache_anomaly_scores(\n                cache_key, result, expire=3600  # 1 hour\n            )\n            \n            ml_logger.info(\n                \"Anomaly detection completed\",\n                commodity_code=commodity_code,\n                total_anomalies=len(anomalies_list),\n                detection_type=detection_type\n            )\n            \n            return result\n            \n        except Exception as e:\n            ml_logger.error(\n                \"Anomaly detection failed\",\n                commodity_code=commodity_code,\n                error=str(e)\n            )\n            raise e\n    \n    async def analyze_price_correlation(\n        self,\n        commodity_code: str,\n        region_codes: List[str] = None,\n        weather_types: List[str] = None,\n        time_window_days: int = 365\n    ) -> Dict:\n        \"\"\"Analyze price-weather correlation.\"\"\"\n        \n        try:\n            # Get price and weather data\n            price_data, weather_data = self.data_processor.load_and_preprocess_data(\n                commodity_codes=[commodity_code],\n                region_codes=region_codes,\n                start_date=(datetime.now() - timedelta(days=time_window_days)).isoformat(),\n                include_weather=True\n            )\n            \n            if price_data.empty or weather_data.empty:\n                return {\n                    'commodity_code': commodity_code,\n                    'correlations': [],\n                    'error': 'Insufficient data for correlation analysis'\n                }\n            \n            # Engineer features for correlation analysis\n            features_df = self.feature_engineer.engineer_features(\n                price_data=price_data,\n                weather_data=weather_data,\n                commodity_code=commodity_code\n            )\n            \n            # Calculate correlations\n            correlations = self._calculate_weather_correlations(\n                features_df, commodity_code, weather_types or ['TEMPERATURE', 'RAINFALL', 'HUMIDITY']\n            )\n            \n            # Generate insights and recommendations\n            insights = self._generate_correlation_insights(correlations)\n            recommendations = self._generate_correlation_recommendations(correlations)\n            \n            result = {\n                'commodity_code': commodity_code,\n                'analysis_period': {\n                    'start': price_data['date'].min().isoformat(),\n                    'end': price_data['date'].max().isoformat()\n                },\n                'correlations': correlations,\n                'summary_insights': insights,\n                'recommendations': recommendations\n            }\n            \n            ml_logger.info(\n                \"Price-weather correlation analysis completed\",\n                commodity_code=commodity_code,\n                correlation_count=len(correlations)\n            )\n            \n            return result\n            \n        except Exception as e:\n            ml_logger.error(\n                \"Correlation analysis failed\",\n                commodity_code=commodity_code,\n                error=str(e)\n            )\n            raise e\n    \n    async def _load_model(self, model_type: str, commodity_code: str, region_code: str = None):\n        \"\"\"Load ML model with caching.\"\"\"\n        \n        model_key = f\"{model_type}_{commodity_code}_{region_code or 'national'}\"\n        \n        # Check memory cache\n        if model_key in self.model_cache:\n            return self.model_cache[model_key]\n        \n        # Load model\n        model = self.model_trainer.load_model(model_type, commodity_code, region_code)\n        \n        # Cache in memory\n        self.model_cache[model_key] = model\n        \n        return model\n    \n    async def _get_recent_data(\n        self,\n        commodity_code: str,\n        region_code: str = None,\n        lookback_days: int = 90\n    ) -> pd.DataFrame:\n        \"\"\"Get recent data for prediction.\"\"\"\n        \n        start_date = (datetime.now() - timedelta(days=lookback_days)).isoformat()\n        \n        price_data, weather_data = self.data_processor.load_and_preprocess_data(\n            commodity_codes=[commodity_code],\n            region_codes=[region_code] if region_code else None,\n            start_date=start_date,\n            include_weather=True\n        )\n        \n        if price_data.empty:\n            raise ValueError(f\"No recent data available for {commodity_code}\")\n        \n        # Engineer features\n        features_df = self.feature_engineer.engineer_features(\n            price_data=price_data,\n            weather_data=weather_data,\n            commodity_code=commodity_code,\n            region_code=region_code\n        )\n        \n        return features_df\n    \n    async def _make_prediction(\n        self,\n        model,\n        data: pd.DataFrame,\n        commodity_code: str,\n        region_code: str,\n        horizon_days: int,\n        model_type: str,\n        include_uncertainty: bool,\n        include_features: bool\n    ) -> Dict:\n        \"\"\"Make prediction using loaded model.\"\"\"\n        \n        # Make prediction based on model type\n        if model_type in ['prophet']:\n            forecast = model.predict(\n                horizon_days=horizon_days,\n                include_history=False\n            )\n        elif model_type in ['lstm']:\n            forecast = model.predict(\n                data,\n                horizon_days=horizon_days\n            )\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n        \n        # Get current price\n        current_price = data['price'].iloc[-1] if not data.empty else 0\n        \n        # Calculate price change forecast\n        if not forecast.empty:\n            future_price = forecast['yhat'].iloc[-1]\n            price_change = ((future_price - current_price) / current_price) * 100\n            \n            # Determine trend direction\n            if price_change > 2:\n                trend_direction = 'up'\n            elif price_change < -2:\n                trend_direction = 'down'\n            else:\n                trend_direction = 'stable'\n        else:\n            future_price = current_price\n            price_change = 0\n            trend_direction = 'stable'\n        \n        # Format predictions\n        predictions = []\n        for _, row in forecast.iterrows():\n            prediction_point = {\n                'date': row['ds'].isoformat() if hasattr(row['ds'], 'isoformat') else str(row['ds']),\n                'predicted_price': float(row['yhat']),\n                'confidence': float(row.get('confidence', 0.8))\n            }\n            \n            if include_uncertainty:\n                prediction_point['lower_bound'] = float(row.get('yhat_lower', row['yhat'] * 0.95))\n                prediction_point['upper_bound'] = float(row.get('yhat_upper', row['yhat'] * 1.05))\n            \n            predictions.append(prediction_point)\n        \n        # Get commodity info\n        commodity_info = db_manager.get_commodities_info()\n        commodity_name = commodity_info[\n            commodity_info['code'] == commodity_code\n        ]['name'].iloc[0] if not commodity_info.empty else commodity_code\n        \n        # Get region info\n        region_name = None\n        if region_code:\n            region_info = db_manager.get_regions_info()\n            region_name = region_info[\n                region_info['code'] == region_code\n            ]['name'].iloc[0] if not region_info.empty else region_code\n        \n        result = {\n            'commodity_code': commodity_code,\n            'commodity_name': commodity_name,\n            'region_code': region_code,\n            'region_name': region_name,\n            'model_type': model_type,\n            'model_version': model.model_metadata.get('training_date', 'unknown'),\n            'predictions': predictions,\n            'current_price': float(current_price),\n            'price_change_forecast': float(price_change),\n            'trend_direction': trend_direction,\n            'prediction_date': datetime.now().isoformat()\n        }\n        \n        # Add feature importance if requested\n        if include_features and hasattr(model, 'feature_columns'):\n            result['features_used'] = model.feature_columns[:10]  # Top 10 features\n            # Add feature importance if available\n            if hasattr(model, 'get_feature_importance'):\n                result['feature_importance'] = model.get_feature_importance()\n        \n        return result\n    \n    def _generate_cache_key(\n        self,\n        commodity_code: str,\n        region_code: str,\n        horizon_days: int,\n        model_type: str\n    ) -> str:\n        \"\"\"Generate cache key for prediction.\"\"\"\n        return f\"pred:{commodity_code}:{region_code or 'national'}:{horizon_days}:{model_type}\"\n    \n    def _update_prediction_stats(self, latency_ms: float):\n        \"\"\"Update prediction statistics.\"\"\"\n        self.prediction_stats['total_predictions'] += 1\n        self.prediction_stats['last_prediction_time'] = datetime.now().isoformat()\n        \n        # Update average latency using exponential moving average\n        if self.prediction_stats['average_latency_ms'] == 0:\n            self.prediction_stats['average_latency_ms'] = latency_ms\n        else:\n            alpha = 0.1  # Smoothing factor\n            self.prediction_stats['average_latency_ms'] = (\n                alpha * latency_ms + \n                (1 - alpha) * self.prediction_stats['average_latency_ms']\n            )\n    \n    def _calculate_anomaly_summary(self, anomalies_df: pd.DataFrame) -> Dict:\n        \"\"\"Calculate summary statistics for anomalies.\"\"\"\n        \n        if anomalies_df.empty:\n            return {\n                'total_anomalies': 0,\n                'severity_distribution': {},\n                'anomaly_type_distribution': {},\n                'average_anomaly_score': 0\n            }\n        \n        summary = {\n            'total_anomalies': len(anomalies_df),\n            'severity_distribution': anomalies_df['severity'].value_counts().to_dict(),\n            'anomaly_type_distribution': anomalies_df['anomaly_type'].value_counts().to_dict(),\n            'average_anomaly_score': float(anomalies_df['anomaly_score'].mean()),\n            'max_anomaly_score': float(anomalies_df['anomaly_score'].max()),\n            'recent_anomalies_count': len(anomalies_df[\n                anomalies_df['date'] >= (datetime.now() - timedelta(days=7)).isoformat()\n            ])\n        }\n        \n        return summary\n    \n    def _calculate_weather_correlations(\n        self,\n        features_df: pd.DataFrame,\n        commodity_code: str,\n        weather_types: List[str]\n    ) -> List[Dict]:\n        \"\"\"Calculate weather-price correlations.\"\"\"\n        \n        correlations = []\n        \n        for weather_type in weather_types:\n            weather_col = f\"weather_{weather_type.lower()}\"\n            \n            if weather_col in features_df.columns and 'price' in features_df.columns:\n                # Calculate correlation\n                corr_coef = features_df[weather_col].corr(features_df['price'])\n                \n                if not np.isnan(corr_coef):\n                    # Determine significance level\n                    abs_corr = abs(corr_coef)\n                    if abs_corr > 0.7:\n                        significance = 'high'\n                    elif abs_corr > 0.5:\n                        significance = 'medium'\n                    elif abs_corr > 0.3:\n                        significance = 'low'\n                    else:\n                        significance = 'not_significant'\n                    \n                    correlations.append({\n                        'commodity_code': commodity_code,\n                        'weather_type': weather_type,\n                        'correlation_coefficient': float(corr_coef),\n                        'p_value': 0.05,  # Simplified\n                        'significance_level': significance,\n                        'lag_days': 0,  # Simplified\n                        'sample_size': len(features_df.dropna(subset=[weather_col, 'price']))\n                    })\n        \n        return correlations\n    \n    def _generate_correlation_insights(self, correlations: List[Dict]) -> List[str]:\n        \"\"\"Generate insights from correlation analysis.\"\"\"\n        \n        insights = []\n        \n        for corr in correlations:\n            if corr['significance_level'] in ['high', 'medium']:\n                direction = 'positif' if corr['correlation_coefficient'] > 0 else 'negatif'\n                insights.append(\n                    f\"{corr['weather_type']} memiliki korelasi {direction} \"\n                    f\"({corr['correlation_coefficient']:.2f}) dengan harga {corr['commodity_code']}\"\n                )\n        \n        if not insights:\n            insights.append(\"Tidak ditemukan korelasi signifikan antara cuaca dan harga\")\n        \n        return insights\n    \n    def _generate_correlation_recommendations(self, correlations: List[Dict]) -> List[str]:\n        \"\"\"Generate recommendations from correlation analysis.\"\"\"\n        \n        recommendations = []\n        \n        significant_correlations = [\n            c for c in correlations if c['significance_level'] in ['high', 'medium']\n        ]\n        \n        if significant_correlations:\n            recommendations.append(\n                \"Monitor kondisi cuaca untuk memprediksi perubahan harga\"\n            )\n            recommendations.append(\n                \"Pertimbangkan data cuaca dalam model prediksi harga\"\n            )\n        \n        return recommendations\n    \n    def get_prediction_stats(self) -> Dict:\n        \"\"\"Get prediction service statistics.\"\"\"\n        \n        cache_hit_rate = (\n            self.prediction_stats['cache_hits'] / \n            max(self.prediction_stats['total_predictions'], 1)\n        ) * 100\n        \n        return {\n            **self.prediction_stats,\n            'cache_hit_rate': cache_hit_rate,\n            'models_loaded': len(self.model_cache)\n        }\n    \n    async def health_check(self) -> Dict:\n        \"\"\"Check service health.\"\"\"\n        \n        try:\n            # Check database connectivity\n            db_healthy = await db_manager.health_check()\n            \n            # Check Redis connectivity\n            redis_healthy = await redis_manager.health_check()\n            \n            # Check model availability\n            models_available = len(self.model_cache) > 0\n            \n            overall_healthy = db_healthy and redis_healthy\n            \n            return {\n                'status': 'healthy' if overall_healthy else 'unhealthy',\n                'database': db_healthy,\n                'redis': redis_healthy,\n                'models_available': models_available,\n                'prediction_stats': self.get_prediction_stats(),\n                'timestamp': datetime.now().isoformat()\n            }\n            \n        except Exception as e:\n            ml_logger.error(\"Health check failed\", error=str(e))\n            return {\n                'status': 'unhealthy',\n                'error': str(e),\n                'timestamp': datetime.now().isoformat()\n            }\n\n\n# Global prediction service instance\nprediction_service = PredictionService()\n